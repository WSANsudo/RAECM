# RAECM - Student Model Distillation

**Student Model Component** of RAECM framework for high-throughput router attribute identification. This module distills knowledge from the teacher-side LLM pipeline into efficient task-specific models, achieving 86.43% accuracy while enabling scalable deployment.

## ğŸ“¦ Module Overview

The student model component addresses the throughput-reliability trade-off in Internet-scale identification:

- **High Throughput**: Local inference without API dependencies
- **Cost Effective**: Eliminates per-request API costs
- **Strong Accuracy**: Maintains strong performance on benchmark dataset
- **Scalable Deployment**: Suitable for continuous monitoring and large-scale analysis

## ğŸ¯ Key Features

### Knowledge Distillation

- **Evidence-Grounded Training**: Learn from teacher's structured outputs with explicit evidence
- **Task-Specific Optimization**: Specialized models for vendor/OS/device type identification
- **Retrieval-Augmented**: Maintains RAG capability for long-tail cases
- **Independent Deployment**: Can run independently without teacher model dependency

### Model Architecture

- **Base Models**: Qwen2.5/Qwen3/Llama3 series
- **LoRA Fine-tuning**: Parameter-efficient adaptation
- **Multi-Task Support**: Vendor, OS, and device type identification
- **Configurable Size**: From 1.5B to 8B parameters

### Deployment Strategy

- **Independent Inference**: Student model handles identification tasks independently
- **Batch Processing**: Efficient parallel inference
- **Quality Control**: Confidence assessment and result verification
- **Large-Scale Deployment**: Suitable for continuous monitoring scenarios

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+ (for GPU training)
- 8GB+ GPU memory (depends on model size)

### Installation

```bash
# Install dependencies
pip install -r requirements.txt
```

Main dependencies:
- torch >= 2.0.0
- transformers >= 4.30.0
- datasets
- accelerate
- peft
- bitsandbytes

### Download Pre-trained Models

```bash
# Using ModelScope (recommended for China)
pip install modelscope
modelscope download --model Qwen/Qwen2.5-3B-Instruct --local_dir ./models/Qwen2.5-3B-Instruct

# Or using Hugging Face
huggingface-cli download Qwen/Qwen2.5-3B-Instruct --local-dir ./models/Qwen2.5-3B-Instruct
```

### Training

```bash
# Train vendor identification model (recommended: Qwen2.5-3B)
python train.py --mt vd --model qwen2.5-3b

# Train OS identification model
python train.py --mt os --model qwen2.5-3b

# Train device type identification model
python train.py --mt dt --model qwen2.5-3b

# Advanced training options
python train.py --mt vd --model qwen2.5-3b \
  --epochs 3 \
  --batch-size 16 \
  --learning-rate 2e-5 \
  --max-length 2048
```

### Evaluation

```bash
# Evaluate trained model
python evaluate.py --mt vd

# Specify model path
python evaluate.py --mt vd --model output/vendor/Qwen2.5-3B/best_model

# Quick evaluation (50 samples)
python evaluate.py --mt vd --max-samples 50
```

### Prediction

```bash
# Single prediction
python predict.py --mt vd \
  --model output/vendor/Qwen2.5-3B/best_model \
  --input "Port 8291 (Winbox), HTTP banner: RouterOS"

# Batch prediction
python predict.py --mt vd \
  --model output/vendor/Qwen2.5-3B/best_model \
  --input-file test_data.jsonl \
  --output-file predictions.jsonl
```

## ğŸ“ Project Structure

### Core Files

```
model/
â”œâ”€â”€ train.py                     # Training entry point
â”œâ”€â”€ evaluate.py                  # Evaluation entry point
â”œâ”€â”€ predict.py                   # Prediction entry point
â”œâ”€â”€ full_run.py                  # Complete pipeline (train + evaluate)
â”œâ”€â”€ config.yaml                  # Global configuration
â”œâ”€â”€ requirements.txt             # Dependencies
â””â”€â”€ README.md                    # This file
```

### Training Core

```
model/training/
â”œâ”€â”€ config.py                    # Configuration management
â”œâ”€â”€ data_processor.py            # Data processing pipeline
â”œâ”€â”€ trainer.py                   # Standard trainer
â”œâ”€â”€ evaluator.py                 # Model evaluation
â”œâ”€â”€ inference.py                 # Inference service
â”œâ”€â”€ simple_classifier.py         # Simplified classification training
â”œâ”€â”€ train_evaluator.py           # Training evaluator
â”œâ”€â”€ metrics_recorder.py          # Metrics recording
â”œâ”€â”€ model_manager.py             # Model management
â”œâ”€â”€ model_presets.py             # Model presets
â”œâ”€â”€ gpu_check.py                 # GPU checking
â”œâ”€â”€ data_pipeline_v2.py          # Data pipeline v2
â”œâ”€â”€ evaluation_v2.py             # Evaluation v2
â”‚
â””â”€â”€ distillation/                # Distillation training module
    â”œâ”€â”€ config.py               # Distillation configuration
    â”œâ”€â”€ trainer.py              # Distillation trainer
    â”œâ”€â”€ datasets.py             # Dataset handling
    â”œâ”€â”€ losses.py               # Loss functions
    â”œâ”€â”€ schedulers.py           # Learning rate schedulers
    â”œâ”€â”€ memory.py               # Memory management
    â””â”€â”€ utils.py                # Utility functions
```

### Model Configurations

```
model/configs/
â”œâ”€â”€ Qwen2.5-1.5B-Instruct.yaml   # Qwen2.5-1.5B config
â”œâ”€â”€ Qwen2.5-3B-Instruct.yaml     # Qwen2.5-3B config (recommended)
â”œâ”€â”€ Qwen2.5-7B-Instruct.yaml     # Qwen2.5-7B config
â”œâ”€â”€ Qwen3-0.6B.yaml              # Qwen3-0.6B config
â”œâ”€â”€ Qwen3-4B.yaml                # Qwen3-4B config
â”œâ”€â”€ Qwen3-8B-Base.yaml           # Qwen3-8B config
â”œâ”€â”€ Llama-3-8B-Instruct.yaml     # Llama3-8B config
â””â”€â”€ ...                          # Other model configs
```

### Training Data

```
model/input/
â”œâ”€â”€ example_train.jsonl          # Example training data
â”œâ”€â”€ vendor_model_train.jsonl     # Vendor identification data
â”œâ”€â”€ os_model_train.jsonl         # OS identification data
â””â”€â”€ devicetype_model_train.jsonl # Device type identification data
```

### Prompt Templates

```
model/prompt/
â”œâ”€â”€ product_prompts.json         # Product identification prompts
â”œâ”€â”€ check_prompts.json           # Verification prompts
â”œâ”€â”€ student.json                 # Student model prompts
â””â”€â”€ new_prompt.json              # New prompts
```

### Training Scripts

```
model/bash/
â”œâ”€â”€ qwen3-0.6b.sh                # Qwen3-0.6B training script
â”œâ”€â”€ qwen3-8b.sh                  # Qwen3-8B training script
â”œâ”€â”€ qwen3-32b.sh                 # Qwen3-32B training script
â””â”€â”€ llama3-8b.sh                 # Llama3-8B training script
```

### Runtime Directories

```
model/
â”œâ”€â”€ models/                      # Pre-trained models (download required)
â”‚   â”œâ”€â”€ Qwen2.5-3B-Instruct/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ output/                      # Training outputs
â”‚   â”œâ”€â”€ vendor/                 # Vendor identification models
â”‚   â”œâ”€â”€ os/                     # OS identification models
â”‚   â””â”€â”€ devicetype/             # Device type identification models
â”‚
â”œâ”€â”€ result/                      # Evaluation results
â”‚   â”œâ”€â”€ evaluation_report_vd.json
â”‚   â”œâ”€â”€ evaluation_report_vd.md
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ logs/                        # Training logs
```

## ğŸ¯ Task Types

| Parameter | Task | Description | Example Labels |
|-----------|------|-------------|----------------|
| `vd` | Vendor Identification | Identify device vendor | MikroTik, Cisco, Juniper |
| `os` | OS Identification | Identify operating system | RouterOS, IOS, JunOS |
| `dt` | Device Type | Identify device type | router, switch, firewall |

## ğŸ’» Recommended Model Configurations

### Qwen2.5 Series (Recommended) â­

| Model | Parameters | Memory | Speed | Use Case |
|-------|-----------|--------|-------|----------|
| qwen2.5-1.5b | 1.5B | ~8GB | Fastest | Quick testing |
| qwen2.5-3b | 3B | ~12GB | Fast | **Production (recommended)** |
| qwen2.5-7b | 7B | ~20GB | Medium | High accuracy requirements |

### Qwen3 Series

| Model | Parameters | Memory | Speed | Use Case |
|-------|-----------|--------|-------|----------|
| qwen3-0.6b | 0.6B | ~6GB | Fastest | Rapid prototyping |
| qwen3-4b | 4B | ~14GB | Fast | Balanced performance |
| qwen3-8b | 8B | ~22GB | Medium | High accuracy |

### Llama3 Series

| Model | Parameters | Memory | Speed | Use Case |
|-------|-----------|--------|-------|----------|
| llama3-8b | 8B | ~22GB | Medium | Comparative experiments |

## ğŸ“Š Training Parameters

### Basic Configuration

```bash
python train.py \
  --mt vd \                    # Task type (vd/os/dt)
  --model qwen2.5-3b \         # Model name
  --epochs 3 \                 # Training epochs
  --batch-size 16 \            # Batch size
  --learning-rate 2e-5 \       # Learning rate
  --max-length 2048            # Maximum sequence length
```

### Advanced Configuration

```bash
python train.py \
  --mt vd \
  --model qwen2.5-3b \
  --lora-r 8 \                 # LoRA rank
  --lora-alpha 16 \            # LoRA alpha
  --lora-dropout 0.05 \        # LoRA dropout
  --warmup-ratio 0.1 \         # Warmup ratio
  --weight-decay 0.01 \        # Weight decay
  --gradient-accumulation 4    # Gradient accumulation steps
```

## ğŸ“ˆ Evaluation

### Basic Evaluation

```bash
# Evaluate vendor identification model
python evaluate.py --mt vd

# Evaluate OS identification model
python evaluate.py --mt os

# Evaluate device type identification model
python evaluate.py --mt dt
```

### Specify Model Path

```bash
python evaluate.py \
  --mt vd \
  --model output/vendor/Qwen2.5-3B/best_model
```

### Evaluation Output

Generates comprehensive reports:
- `result/evaluation_report_vd.json` - JSON format detailed report
- `result/evaluation_report_vd.md` - Markdown format report

Report includes:
- Overall metrics (accuracy, F1 score)
- Per-class metrics (precision, recall, F1)
- Confusion matrix
- Label distribution
- Error analysis

## ğŸ”® Model Prediction

### Single Prediction

```bash
python predict.py \
  --mt vd \
  --model output/vendor/Qwen2.5-3B/best_model \
  --input "Port 8291 (Winbox), HTTP banner: RouterOS"
```

### Batch Prediction

```bash
python predict.py \
  --mt vd \
  --model output/vendor/Qwen2.5-3B/best_model \
  --input-file test_data.jsonl \
  --output-file predictions.jsonl
```

## ğŸ›ï¸ Configuration Files

### Global Configuration (config.yaml)

```yaml
# Training configuration
training:
  epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  max_length: 2048
  
# LoRA configuration
lora:
  r: 8
  alpha: 16
  dropout: 0.05
  
# Data configuration
data:
  train_ratio: 0.8
  valid_ratio: 0.1
  test_ratio: 0.1
```

### Model Configuration (configs/*.yaml)

Each model has its own configuration file containing:
- Model path
- Tokenizer configuration
- Training parameters
- LoRA parameters

## ğŸš€ Complete Training Pipeline

Use `full_run.py` for end-to-end training-evaluation-prediction:

```bash
python full_run.py --mt vd --model qwen2.5-3b
```

Pipeline includes:
1. Data preprocessing
2. Model training
3. Model evaluation
4. Report generation
5. Best model saving

## ğŸ“Š Data Formats

### Training Data (JSONL)

```json
{
  "input": "Port 8291 (Winbox), HTTP banner: RouterOS v6.49",
  "output": "MikroTik",
  "Services": [...]
}
```

### Prediction Input (JSONL)

```json
{
  "IP Index": "192.168.1.1",
  "Services": [
    {
      "Port": 8291,
      "Protocol": "Winbox",
      "Banner": "RouterOS v6.49"
    }
  ]
}
```

### Prediction Output (JSONL)

```json
{
  "IP Index": "192.168.1.1",
  "Vendor": "MikroTik",
  "Confidence": 0.95,
  "Services": [...]
}
```

## ğŸ“Š Data Processing Pipeline

```
Labeled Data (input/*.jsonl)
    â†“
Data Processing (data_processor.py)
    â”œâ”€ Format conversion
    â”œâ”€ Train/valid/test split
    â””â”€ Tokenization
    â†“
Model Training (trainer.py)
    â”œâ”€ LoRA fine-tuning
    â”œâ”€ Gradient accumulation
    â””â”€ Checkpoint saving
    â†“
Model Evaluation (evaluator.py)
    â”œâ”€ Accuracy calculation
    â”œâ”€ Per-class metrics
    â””â”€ Confusion matrix
    â†“
Trained Model (output/*/best_model)
```

## ğŸ¯ Use Cases

### Scenario 1: High-Throughput Identification

```
Scanning Data â†’ Student Model â†’ Rapid Classification
```

1. Deploy trained student model
2. Process observations in batches
3. Achieve high throughput with maintained accuracy

### Scenario 2: Continuous Monitoring

```
Periodic Scanning â†’ Student Model â†’ Longitudinal Analysis
```

1. Deploy student model for continuous monitoring
2. Track attribute changes over time
3. Cost-effective large-scale analysis

## â“ Frequently Asked Questions

### Q1: How much training data is needed?

Recommended data volume:
- **Minimum**: Sufficient samples per class for model convergence
- **Recommended**: Adequate samples per class for robust performance
- **Ideal**: Rich dataset with diverse samples per class

### Q2: How long does training take?

Training time depends on:
- Model size (smaller models train faster)
- Data volume (more data requires longer training)
- GPU performance (better GPUs reduce training time)

### Q3: How to handle GPU memory issues?

Solutions:
- Reduce `batch_size` (e.g., from 16 to 8)
- Reduce `max_length` (e.g., from 2048 to 1024)
- Use smaller model (e.g., qwen2.5-1.5b)
- Enable gradient accumulation

### Q4: How to improve model accuracy?

Strategies:
- Increase training data
- Increase training epochs
- Adjust learning rate
- Use larger model
- Tune LoRA parameters

### Q5: How to optimize inference speed?

Optimizations:
- Use smaller model (qwen2.5-1.5b)
- Increase batch size
- Use GPU inference
- Enable mixed precision (fp16/bf16)

### Q6: What is the accuracy-cost trade-off?

| Strategy | Accuracy | Cost | Throughput |
|----------|----------|------|------------|
| Teacher | Highest | High | Low |
| Student | Strong | Low | High |

### Q7: What is the relationship between student model and teacher pipeline?

The student model learns from the teacher pipeline through knowledge distillation:
- Compatible input/output formats
- Can be deployed independently
- Shared task definitions and label systems
- Consistent evaluation metrics

## ğŸ”¬ Performance Metrics

Based on benchmark evaluation:

| Metric | Teacher | Student |
|--------|---------|---------|
| Accuracy | Highest | Strong |
| Throughput | Low | High |
| Cost | High | Low |
| Latency | High | Low |

## ğŸ’¡ Best Practices

### Model Selection

- **Quick testing**: qwen2.5-1.5b or qwen3-0.6b
- **Production deployment**: qwen2.5-3b (recommended)
- **High accuracy requirements**: qwen2.5-7b or qwen3-8b

### Training Parameters

- **Learning rate**: Typically in the range of 1e-5 to 2e-5
- **Batch size**: Start with 16 (increase if memory allows)
- **Training epochs**: Usually 3-5 epochs (adjust based on validation performance)

### Data Preparation

- Ensure data quality (clean, deduplicated)
- Balance class distribution
- Split train/valid/test appropriately

### Evaluation Metrics

- Focus on F1 score (balances precision and recall)
- Check confusion matrix (identify confusable classes)
- Analyze error samples (improve data or model)

## ğŸ“š Related Documentation

- [training/config.py](training/config.py) - Training configuration
- [training/trainer.py](training/trainer.py) - Trainer implementation
- [training/evaluator.py](training/evaluator.py) - Evaluator implementation
- [../README.md](../README.md) - Project overview
- [../recog/README.md](../recog/README.md) - Teacher pipeline


