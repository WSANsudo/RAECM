# Qwen3-32B 配置
# 显存: 80GB (紧张)
# 数据量: ~1400 样本
# 策略: 超大模型 + 最小 batch + 极保守学习率 + 最严格防过拟合

model:
  name_or_path: "./models/Qwen3-32B"
  type: "qwen3"
  use_lora: true
  
  # LoRA 配置 - 超大模型用最小 rank
  lora_rank: 4
  lora_alpha: 8
  lora_dropout: 0.2
  lora_target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  trust_remote_code: true

training:
  output_dir: "./output"
  
  num_train_epochs: 10
  
  # 80GB 显存，32B 模型需要最小 batch
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16  # 有效 batch = 16
  
  # 学习率 - 超大模型极保守
  learning_rate: 8.0e-6
  warmup_ratio: 0.2
  lr_scheduler_type: "cosine"
  
  # 正则化 - 最严格
  weight_decay: 0.1
  max_grad_norm: 0.3
  
  # 评估和保存
  logging_steps: 30
  save_steps: 150
  eval_steps: 60
  save_total_limit: 2
  
  # 精度
  bf16: true
  fp16: false
  gradient_checkpointing: true  # 必须开启
  
  optim: "adamw_torch"
  dataloader_num_workers: 2
  dataloader_pin_memory: true

data:
  output_dir: "./data"
  train_ratio: 0.8
  valid_ratio: 0.1
  test_ratio: 0.1
  augment_ratio: 0.0

inference:
  temperature: 0.1
  top_p: 0.9
  max_new_tokens: 128
  do_sample: false
  repetition_penalty: 1.2

distillation:
  lambda_struct: 1.0
  lambda_cons: 0.0
  lambda_pref: 0.0
  lambda_feat: 0.0
  use_dpo: false
  
  # 早停配置 - 超大模型最严格
  early_stopping: true
  early_stopping_patience: 3
  early_stopping_min_delta: 0.001
  min_epochs: 5
  
  max_new_tokens: 50
  eval_samples: 200
  lr_scheduler_type: "cosine"
  
  collect_errors: false
