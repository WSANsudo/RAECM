# Qwen2.5-1.5B-Instruct 配置
# 显存: 80GB (充足)
# 数据量: ~1400 样本
# 策略: 小模型 + 大 batch + 较高学习率

model:
  name_or_path: "./models/Qwen2.5-1.5B-Instruct"
  type: "qwen2"
  use_lora: true
  
  # LoRA 配置 - 小模型用较大 rank
  lora_rank: 32
  lora_alpha: 64
  lora_dropout: 0.1
  lora_target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  trust_remote_code: true

training:
  output_dir: "./output"
  
  num_train_epochs: 20
  
  # 80GB 显存，小模型可以用大 batch
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2  # 有效 batch = 32
  
  # 学习率
  learning_rate: 5.0e-5
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  
  # 正则化
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # 评估和保存
  logging_steps: 10
  save_steps: 50
  eval_steps: 25
  save_total_limit: 3
  
  # 精度
  bf16: true
  fp16: false
  gradient_checkpointing: false
  
  optim: "adamw_torch"
  dataloader_num_workers: 4
  dataloader_pin_memory: true

data:
  output_dir: "./data"
  train_ratio: 0.8
  valid_ratio: 0.1
  test_ratio: 0.1
  augment_ratio: 0.0

inference:
  temperature: 0.1
  top_p: 0.9
  max_new_tokens: 128
  do_sample: false
  repetition_penalty: 1.1

distillation:
  lambda_struct: 1.0
  lambda_cons: 0.0
  lambda_pref: 0.0
  lambda_feat: 0.0
  use_dpo: false
  
  # 早停配置
  early_stopping: true
  early_stopping_patience: 3
  early_stopping_min_delta: 0.001
  min_epochs: 5
  
  max_new_tokens: 50
  eval_samples: 300
  lr_scheduler_type: "cosine"
  
  collect_errors: false
