  # Llama-3.2-1B 配置 - 精准稳妥训练策略
# 显存: 80GB (充足)
# 数据量: ~1400 样本
# 策略: 30轮训练 + 缓慢学习率 + 平缓调度 + 保守参数

model:
  name_or_path: "./models/Llama-3.2-1B"
  type: "llama3"
  use_lora: true
  
  # LoRA 配置 - 适中的秩，避免过拟合
  lora_rank: 12
  lora_alpha: 24
  lora_dropout: 0.15
  lora_target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  trust_remote_code: true

training:
  output_dir: "./output"
  
  # 30轮训练 - 充分学习
  num_train_epochs: 30
  
  # 80GB 显存充足 - 适中的 batch size
  per_device_train_batch_size: 6
  per_device_eval_batch_size: 6
  gradient_accumulation_steps: 5  # 有效 batch = 30
  
  # 缓慢学习率 - 精准稳妥
  learning_rate: 1.0e-5  # 更低的学习率，极其稳定
  warmup_ratio: 0.15     # 更长的 warmup (15% = 4.5 epochs)
  lr_scheduler_type: "cosine"  # 平缓的余弦衰减
  
  # 正则化 - 防止过拟合
  weight_decay: 0.05     # 增加权重衰减
  max_grad_norm: 0.8     # 梯度裁剪
  
  # 评估和保存 - 更频繁的检查点
  logging_steps: 10      # 更频繁的日志
  save_steps: 50         # 更频繁的保存
  eval_steps: 25         # 更频繁的评估
  save_total_limit: 5    # 保留更多检查点
  
  # 精度
  bf16: true
  fp16: false
  gradient_checkpointing: false  # 小模型不需要
  
  optim: "adamw_torch"
  dataloader_num_workers: 4
  dataloader_pin_memory: true

data:
  output_dir: "./data"
  train_ratio: 0.8
  valid_ratio: 0.1
  test_ratio: 0.1
  augment_ratio: 0.0

inference:
  temperature: 0.1
  top_p: 0.9
  max_new_tokens: 128
  do_sample: false
  repetition_penalty: 1.1

distillation:
  lambda_struct: 1.0
  lambda_cons: 0.0
  lambda_pref: 0.0
  lambda_feat: 0.0
  use_dpo: false
  
  # 早停配置 - 更宽松的早停
  early_stopping: true
  early_stopping_patience: 5  # 增加耐心值
  early_stopping_min_delta: 0.0005  # 降低最小改进阈值
  min_epochs: 10  # 至少训练10轮
  
  max_new_tokens: 50
  eval_samples: 300
  lr_scheduler_type: "cosine"
  
  collect_errors: false
